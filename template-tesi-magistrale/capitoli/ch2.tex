\chapter{Reinforcement Learning}
\label{chap:reinforcement-learning}

Il \textit{Reinforcement Learning} (RL) è un paradigma computazionale di Machine Learning focalizzato su come agenti software debbano intraprendere azioni in un ambiente dinamico per massimizzare una nozione di ricompensa cumulativa.
Il RL non è caratterizzato da una specifica tecnica o algoritmo, ma da una particolare classe di problemi: quelli in cui l'apprendimento avviene attraverso l'interazione diretta (\textit{trial-and-error}) piuttosto che tramite istruzione passiva. \cite{Sutton2018}

L'idea alla base del RL trae ispirazione dalla psicologia comportamentale: un agente apprende a mappare le situazioni in azioni in modo da massimizzare un segnale di feedback numerico. A differenza di altri approcci di apprendimento automatico, l'agente non viene informato a priori su quale sia l'azione corretta da intraprendere; deve invece scoprirlo tentando diverse strategie e osservandone le conseguenze nel tempo.

In questo capitolo verrà analizzato lo stato dell'arte dell'Apprendimento Per Rinforzo, partendo dalla distinzione rispetto agli altri paradigmi di apprendimento, verranno formalizzati i fondamenti matematici dei Processi Decisionali di Markov (MDP) e analizzate le principali famiglie di algoritmi.

\section{Paradigmi di Apprendimento Automatico}
\label{sec:paradigms}

Per comprendere la natura del RL, è fondamentale distinguerlo dagli altri paradigmi dell'intelligenza artificiale. \cite{Fayaz2022}

\subsection{Apprendimento Supervisionato}

Nell'\textit{Apprendimento Supervisionato} (\textit{Supervised Learning}), il modello viene addestrato su un dataset statico di esempi etichettati $\mathcal{D} = \{(x_i, y_i)\}$, dove per ogni input $x_i$ è fornita la risposta corretta $y_i$ da un supervisore esterno (\textit{Teacher}). L'obiettivo è minimizzare l'errore tra la predizione del modello e l'etichetta reale.
Questo approccio è dominante nei problemi di classificazione e regressione, ma mostra limiti nei problemi decisionali sequenziali, dove non esiste un'etichetta corretta immediata e le azioni influenzano i dati futuri.

\subsection{Apprendimento Non Supervisionato}

L'\textit{Apprendimento Non Supervisionato} (\textit{Unsupervised Learning}) opera su dati privi di etichette $\mathcal{D} = \{x_i\}$. L'obiettivo non è massimizzare una ricompensa o minimizzare un errore rispetto a un target, ma scoprire strutture nascoste nei dati (es. clustering, riduzione dimensionale).

\subsection{Apprendimento Per Rinforzo (Reinforcement Learning)}

L'\textit{Apprendimento Per Rinforzo} (\textit{Reinforcement Learning}) può essere visto come un \textit{apprendimento con un critico}: il critico non dice all'agente cosa avrebbe dovuto fare (come nel supervisionato), ma fornisce solo una valutazione scalare su quanto l'azione scelta sia stata buona o cattiva.
Questo introduce due sfide \cite{Arulkumaran2017}:

\begin{itemize}
    \item \textbf{Exploration vs Exploitation}: l'agente deve bilanciare lo sfruttamento delle azioni note come efficaci con l'esplorazione di nuove azioni potenzialmente migliori.
    \item \textbf{Credit Assignment}: la ricompensa per un'azione può arrivare con un ritardo temporale significativo, rendendo difficile capire quale mossa passata sia responsabile del successo.
\end{itemize}

\section{Formalizzazione del problema}
\label{sec:formalizzazione_rl}

Il Reinforcement Learning è fondato sul \textit{Processo Decisionale di Markov} (MDP). Un MDP fornisce l'astrazione formale per modellare il processo decisionale in situazioni in cui gli esiti sono parzialmente casuali e sotto il controllo di un decisore. \cite{Sutton2018}

Formalmente, un MDP è definito dalla tupla $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$. L'apprendimento avviene  attraverso un ciclo di interazione continuo a passi discreti $t = 0, 1, 2, \dots$. In ogni istante, l'agente osserva lo stato $S_t \in \mathcal{S}$, seleziona un'azione $A_t \in \mathcal{A}$, e l'ambiente risponde transitando in $S_{t+1} \sim \mathcal{P}(\cdot|S_t, A_t)$ ed emettendo una ricompensa $R_{t+1}$.

\subsection{Entità del Sistema}
\label{subsec:entità_rl}

\subsubsection{Agente}

L'agente è l'entità computazionale autonoma che apprende e prende decisioni. È colui che percepisce lo stato del mondo, esegue computazioni e seleziona le azioni da compiere. Non ha controllo diretto sulle dinamiche interne dell'ambiente, ma può influenzarne lo stato futuro attraverso le sue azioni.

\subsubsection{Ambiente}

L'ambiente rappresenta tutto ciò che è esterno all'agente. È un sistema dinamico che reagisce alle sollecitazioni.
Formalmente, l'ambiente riceve l'azione $A_t$ al tempo $t$, aggiorna il proprio stato interno passando da $S_t$ a $S_{t+1}$ secondo una funzione di transizione di probabilità $\mathcal{P}$, e restituisce all'agente una nuova osservazione e un segnale di ricompensa $R_{t+1}$.

\subsection{Spazi di definizione}

La complessità di un problema di RL è determinata dalla natura topologica e dalla cardinalità degli spazi su cui operano le variabili del sistema.

\subsubsection{Stato}

Lo stato (\textit{state}) $S_t$ è una descrizione completa, fedele e priva di ambiguità della configurazione dell'ambiente al tempo $t$. Se l'agente ha accesso allo stato $S_t$, il processo gode della Proprietà di Markov:

\begin{equation}
    \mathbb{P}[S_{t+1} | S_t, A_t] = \mathbb{P}[S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \dots, S_0]
\end{equation}

Questo implica che il futuro dipende esclusivamente dallo stato presente e dall'azione intrapresa, rendendo irrilevante la storia passata. In questo caso ideale, il problema è un MDP.

\subsubsection{Osservazione}

L'osservazione (\textit{observation}) $O_t$ è una visione parziale, rumorosa o incompleta dello stato. In scenari reali, l'agente raramente conosce lo stato vero (es. le preferenze latenti di un utente o i pesi interni di un modello vittima black-box).
In questo caso, il problema è definito come \textit{Partially Observable Markov Decision Process} (POMDP). \cite{Kaelbling1998} Per risolvere un POMDP, l'agente deve spesso ricostruire uno "stato di credenza" o una rappresentazione sufficiente aggregando la cronologia delle osservazioni passate (es. usando LSTM o Mamba).

\subsubsection{Azione}

L'insieme $\mathcal{A}$ di tutte le decisioni valide che l'agente può intraprendere. La natura di questo spazio determina la famiglia di algoritmi utilizzabili.

\paragraph*{Spazi Discreti} ~\\

L'insieme delle azioni è finito e numerabile ($|\mathcal{A}| = K$). È il caso tipico della selezione di un item da un catalogo o di un movimento su griglia. Algoritmi \textit{Value-Based} (es. DQN) sono progettati specificamente per questo scenario, calcolando un valore $Q$ per ogni possibile azione discreta. Tuttavia, nei RS si presenta la sfida del \textit{Large Discrete Action Space} (milioni di item), che richiede accorgimenti specifici. \cite{Dulac-Arnold2016}

\paragraph*{Spazi Continui} ~\\

L'azione è un vettore di numeri reali continui (es. $A_t \in \mathbb{R}^n$). In questo caso non è possibile enumerare tutte le azioni per calcolarne il massimo. Si utilizzano algoritmi di tipo \textit{Policy Gradient} o \textit{Actor-Critic} (es. PPO, DDPG) che possono generare direttamente output continui.

\subsection{Segnali di Feedback e Obiettivi}
\subsubsection{Ricompensa}

La ricompensa (\textit{reward}) $R_t$  è un segnale scalare $R_t \in \mathbb{R}$ che l'ambiente invia all'agente. Essa rappresenta una valutazione immediata della bontà dell'evento accaduto.
La funzione di ricompensa attesa è definita come:

\begin{equation}
    \mathcal{R}(s, a) = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]
\end{equation}

Secondo la \textit{Reward Hypothesis}, tutti gli obiettivi dell'agente possono essere descritti come la massimizzazione della somma cumulativa di questi segnali.

\subsubsection{Ritorno e Orizzonte}

L'agente non mira a massimizzare la ricompensa immediata, ma il \textit{Ritorno Atteso} ($G_t$), ovvero la somma delle ricompense future lungo la traiettoria. Per garantire la convergenza matematica in task infiniti e modellare l'incertezza sul futuro, si introduce il \textit{Fattore di Sconto} $\gamma \in [0, 1]$:

\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

Il parametro $\gamma$ determina l'orizzonte temporale dell'agente:

\begin{itemize}
    \item Se $\gamma \to 0$, l'agente è "miope" (\textit{myopic}): si preoccupa solo della ricompensa immediata.
    \item Se $\gamma \to 1$, l'agente è "lungimirante" (\textit{far-sighted}): le ricompense future hanno un peso paragonabile a quelle presenti.
\end{itemize}

\subsection{Meccanismi Decisionali}

\subsubsection{Policy}

La policy $\pi$ è il "cervello" dell'agente: è la funzione che mappa lo stato percepito nell'azione da intraprendere. È l'elemento che viene modificato durante l'apprendimento.

\paragraph*{Policy Deterministica} ~\\

Definita come $a = \pi(s)$. Per ogni stato, l'agente sceglie sempre la stessa azione. È tipica degli agenti greedy che sfruttano puramente la conoscenza acquisita o di algoritmi come DDPG.

\paragraph*{Policy Stocastica} ~\\

Definita come una distribuzione di probabilità $\pi(a|s) = \mathbb{P}[A_t = a \mid S_t = s]$. L'agente non sceglie l'azione direttamente, ma la campiona da questa distribuzione. Questo approccio è fondamentale per garantire l'\textit{Esplorazione}: permette all'agente di provare azioni diverse nello stesso stato per scoprire nuove strategie potenzialmente migliori.

\section{Funzioni di Valore ed Equazioni di Bellman}
\label{sec:value-function_bellman_equation}

Per risolvere l'MDP, l'agente non può limitarsi a massimizzare la ricompensa immediata, ma deve stimare il ritorno atteso a lungo termine. Questo concetto è formalizzato dalle \textit{Funzioni di Valore} (Value-Function), che costituiscono il cuore degli algoritmi di Reinforcement Learning.

\subsection{Funzioni di valore}
\label{sec:value-function}

Esistono due tipi fondamentali di funzioni, che dipendono dalla policy $\pi$ seguita dall'agente.

\subsubsection{State-Value Function $V_\pi(s)$}

Rappresenta la bontà di trovarsi in un certo stato. È il ritorno atteso partendo dallo stato $s$ e seguendo la policy $\pi$ da quel momento in poi:

\begin{equation}
    V_\pi(s) = \mathbb{E}_\pi [G_t \mid S_t = s]
\end{equation}

\subsubsection{Action-Value Function $Q_\pi(s, a)$}

Rappresenta la bontà di eseguire una specifica azione in un certo stato. È il ritorno atteso partendo da $s$, eseguendo l'azione $a$ e \textit{poi} seguendo la policy $\pi$:

\begin{equation}
    Q_\pi(s, a) = \mathbb{E}_\pi [G_t \mid S_t = s, A_t = a]
\end{equation}

Questa funzione è fondamentale per gli algoritmi \textit{Model-Free}, poiché permette di scegliere l'azione migliore senza dover conoscere la funzione di transizione dell'ambiente.

\subsection{Equazioni di Bellman}
\label{subsec:bellman_equation}

Le funzioni di valore non sono indipendenti, ma soddisfano delle relazioni ricorsive note come \textit{Equazioni di Bellman}. Esse esprimono il valore di uno stato in funzione del valore degli stati successivi. \cite{Sutton2018}

Per la funzione $Q_\pi$, l'equazione è:

\begin{equation}
    Q_\pi(s, a) = \mathbb{E} [R_{t+1} + \gamma Q_\pi(S_{t+1}, A_{t+1}) \mid S_t=s, A_t=a]
\end{equation}

Questa equazione è alla base dei metodi di apprendimento temporale: l'agente può aggiornare la sua stima attuale basandosi sulla stima dello stato futuro (bootstrapping).

\subsection{Ottimalità}
\label{subsec:ottimalità}

L'obiettivo del RL è trovare una \textit{Policy Ottimale} $\pi^*$ che garantisca un ritorno atteso maggiore o uguale a qualsiasi altra policy per tutti gli stati ($Q_{\pi^*}(s,a) \ge Q_\pi(s,a), \forall \pi$).

\subsubsection{Equazione di Ottimalità di Bellman}

Sotto la policy ottimale, il valore di un'azione è uguale alla ricompensa immediata più il valore scontato della \textit{migliore} azione possibile nello stato successivo:

\begin{equation}
    Q^*(s, a) = \mathbb{E} [R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') \mid S_t=s, A_t=a]
\end{equation}

\section{Tassonomia degli Algoritmi}
\label{sec:rl_taxonomy}

Esistono diversi approcci per risolvere il problema dell'apprendimento decisionale sequenziale. La letteratura scientifica classifica gli algoritmi di Reinforcement Learning lungo assi fondamentali che ne determinano l'efficienza, la stabilità e l'applicabilità a specifici domini applicativi. \cite{Arulkumaran2017}

\subsection{Presenza del Modello}
\label{subsec:presenza-modello}

La prima distinzione fondamentale riguarda la conoscenza che l'agente possiede, o cerca di acquisire, sulle dinamiche interne dell'ambiente.

\subsubsection{Model-Based RL}

In questo paradigma, l'agente tenta di apprendere un \textit{modello interno} dell'ambiente che approssimi la funzione di transizione $\mathcal{P}(s'|s,a)$ e la funzione di ricompensa $\mathcal{R}(s,a)$.
Una volta appreso tale modello, l'agente può utilizzarlo per effettuare operazioni di \textit{Planning} (pianificazione): esso simula mentalmente le conseguenze delle azioni future, esplorando possibili traiettorie senza doverle eseguire realmente nell'ambiente fisico.

Il vantaggio principale risiede nell'alta \textit{Sample Efficiency}: l'agente può apprendere strategie efficaci con un numero ridotto di interazioni reali. Tuttavia, il metodo soffre del problema del \textit{Model Bias}: se il modello appreso è impreciso, l'agente pianificherà strategie ottimali per il modello simulato ma disastrose per l'ambiente reale.
In contesti complessi come i sistemi di raccomandazione, modellare esplicitamente la dinamica stocastica del comportamento utente è estremamente difficile. Esempi di algoritmi in questa categoria sono \textit{Dyna-Q} e \textit{Monte Carlo Tree Search} (MCTS).

\subsubsection{Model-Free RL}

L'approccio \textit{Model-Free} rinuncia a costruire un modello esplicito della fisica dell'ambiente. L'agente apprende direttamente la Policy o la Value Function interagendo con l'ambiente reale attraverso un processo di \textit{trial-and-error}.

Sebbene richieda un numero di interazioni maggiore (\textit{Sample Inefficiency}), garantisce una minore complessità computazionale e una maggiore robustezza, poiché non dipende dall'accuratezza di un modello stimato. Grazie alla capacità di scalare su problemi complessi, il Model-Free RL rappresenta l'approccio dominante nel Deep Reinforcement Learning. Le famiglie di algoritmi più note in questa categoria includono \textit{Q-Learning}, \textit{DQN} e \textit{PPO}.

\subsection{Oggetto dell'Apprendimento}
\label{subsec:oggetto_apprendimento}

Una seconda classificazione riguarda l'oggetto matematico che viene approssimato dalla rete neurale per risolvere il problema di controllo.

\subsubsection{Value-Based Methods}

Negli algoritmi \textit{Value-Based}, l'agente non apprende direttamente una strategia di comportamento, ma si concentra sulla stima di una funzione valore (tipicamente $Q(s, a)$) che predice il ritorno atteso. La policy è implicita e viene derivata selezionando l'azione che massimizza il valore stimato:
\begin{equation}
    a_t = \arg\max_{a \in \mathcal{A}} Q(s_t, a)
\end{equation}
Questa famiglia, rappresentata principalmente da \textit{DQN}, è stabile ed efficace in ambienti con spazi di azione discreti (come la scelta di un item). Tuttavia, non può gestire nativamente spazi di azione continui, dove la massimizzazione diventerebbe intrattabile.

\subsubsection{Policy-Based Methods}

Nei metodi \textit{Policy-Based}, l'agente apprende direttamente una funzione di policy parametrizzata $\pi_\theta(a|s)$ che mappa lo stato in una distribuzione di probabilità sulle azioni. L'ottimizzazione avviene tramite ascesa del gradiente sulla funzione obiettivo $J(\theta)$ (ritorno atteso):
\begin{equation}
    \theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t)
\end{equation}
Il vantaggio principale è la capacità di apprendere policy stocastiche e gestire spazi di azione continui. Tuttavia, soffrono spesso di alta varianza nella stima del gradiente, portando a una convergenza lenta.

\subsubsection{Actor-Critic Methods}

I metodi \textit{Actor-Critic} combinano i punti di forza dei due approcci precedenti. L'\textbf{Attore} (Policy-Based) seleziona le azioni, mentre il \textbf{Critico} (Value-Based) stima la funzione valore per giudicare la bontà delle azioni scelte.
Il Critico fornisce un segnale di feedback a bassa varianza per guidare l'aggiornamento dell'Attore, stabilizzando l'apprendimento. Esempi di questa famiglia sono \textit{A2C}, \textit{A3C} e \textit{PPO}.

\subsection{Dinamiche di Apprendimento}
\label{subsec:dinamiche_apprendimento}

Questa distinzione riguarda la relazione tra la strategia usata per esplorare e quella che viene ottimizzata.

\subsubsection{On-Policy Learning}

Nel paradigma \textit{On-Policy}, l'agente valuta e migliora la stessa policy $\pi$ che sta utilizzando per interagire con l'ambiente.
I campioni utilizzati per il training devono essere stati generati dalla policy corrente; una volta aggiornati i pesi, le esperienze passate non sono più valide e devono essere scartate. Di conseguenza, algoritmi come \textit{PPO} o \textit{SARSA} tendono ad avere una bassa efficienza dei campioni, richiedendo molte interazioni fresche.

\subsubsection{Off-Policy Learning}

Nel paradigma \textit{Off-Policy}, l'agente apprende il valore della policy ottima $\pi^*$ indipendentemente dalla policy comportamentale $\mu$ usata per generare i dati.
Questo disaccoppiamento permette l'utilizzo dell'\textit{Experience Replay}: l'agente memorizza le transizioni passate in un buffer $\mathcal{D}$ e le riutilizza più volte per il training. Grazie a questa caratteristica, algoritmi come \textit{DQN} o \textit{SAC} risultano estremamente efficienti nell'uso dei dati, rendendoli preferibili in contesti dove l'interazione è costosa, come nei sistemi di raccomandazione reali.

\subsection{Modalità di Interazione}
\label{subsec:online_offline_rl}

Un'ultima classificazione critica, specialmente per le applicazioni ai Sistemi di Raccomandazione, riguarda la modalità di interazione con l'ambiente durante la fase di addestramento. La scelta tra apprendere interagendo in tempo reale o apprendere da dati storici statici determina non solo l'architettura del sistema, ma anche la sua sicurezza e applicabilità in contesti produttivi.

\subsubsection{Online Reinforcement Learning}

Nel paradigma classico \textit{Online}, l'apprendimento e la raccolta dati avvengono contemporaneamente in un ciclo continuo. L'agente interagisce con l'ambiente in tempo reale, osserva le conseguenze delle sue azioni e aggiorna immediatamente la sua policy per influenzare le decisioni future.
Questo approccio permette all'agente di esplorare attivamente stati nuovi e correggere i propri errori, adattandosi dinamicamente ai cambiamenti della distribuzione dei dati. Tuttavia, l'applicazione diretta nei sistemi di raccomandazione reali presenta rischi significativi. Lasciare che un agente esplori casualmente (o secondo una policy non ancora ottimizzata) significa mostrare raccomandazioni di bassa qualità agli utenti reali, degradando l'esperienza utente e causando potenziali perdite economiche. Inoltre, il ciclo di feedback in un sistema live può essere estremamente lento, rallentando la convergenza dell'algoritmo.

\subsubsection{Offline (Batch) Reinforcement Learning}

Per ovviare ai rischi dell'interazione live, il paradigma \textit{Offline} (o \textit{Batch}) \textit{Reinforcement Learning} disaccoppia completamente l'apprendimento dall'esecuzione. In questo scenario, l'agente non ha accesso all'ambiente durante il training, ma deve apprendere esclusivamente da un dataset statico di transizioni storiche (log di interazioni passate) raccolte in precedenza da un'altra policy, spesso sub-ottimale o euristica.
Sebbene questo approccio sia più sicuro ed economico, permettendo di sfruttare gli enormi volumi di dati storici già disponibili, esso introduce una sfida teorica complessa nota come \textit{Distributional Shift}. \cite{Levine2020} Il problema sorge quando l'agente, nel tentativo di migliorare rispetto alla policy storica, interroga la funzione $Q$ su azioni che non sono mai state eseguite nel dataset (azioni \textit{Out-Of-Distribution}). Gli algoritmi standard come DQN tendono a sovrastimare enormemente il valore di queste azioni sconosciute, portando l'agente ad apprendere policy fallimentari se non vengono applicate specifiche tecniche di regolarizzazione o vincoli di conservatività.

\section{Algoritmi Model-Based}
\label{sec:model_based}

La classe degli algoritmi Model-Based si fonda sull'apprendimento (o sulla conoscenza a priori) di un modello interno delle dinamiche dell'ambiente. Mentre i metodi Model-Free ignorano la struttura sottostante del sistema, un agente Model-Based cerca di approssimare due funzioni fondamentali che definiscono l'MDP:

\begin{enumerate}
    \item \textbf{Transition Model $\hat{\mathcal{P}}(s'|s,a)$}: predice la distribuzione del prossimo stato dato lo stato corrente e l'azione.
    \item \textbf{Reward Model $\hat{\mathcal{R}}(s,a)$}: predice la ricompensa immediata attesa.
\end{enumerate}

Una volta appreso il modello, l'agente può utilizzarlo per generare esperienze simulate (spesso chiamate "sogni") e per pianificare (\textit{Planning}) le azioni future. Il vantaggio principale risiede nella \textit{Sample Efficiency}: l'agente può migliorare la sua policy effettuando milioni di simulazioni mentali per ogni singola interazione costosa con l'ambiente reale.

\subsection{Dyna-Q}
\label{subsec:Dyna-Q}

L'architettura Dyna-Q è il primo framework a unificare l'apprendimento diretto (Model-Free) con la pianificazione. L'algoritmo mantiene una struttura dati $Model(s, a)$ che memorizza l'ultimo risultato osservato $(R, S')$ per ogni coppia stato-azione visitata.

Il ciclo di apprendimento ad ogni step $t$ avviene in due fasi distinte:

\begin{enumerate}
    \item \textbf{Direct RL (Learning)}: l'agente esegue un'azione reale nell'ambiente, osserva la transizione e usa questo dato per aggiornare la funzione $Q(s,a)$ (usando la regola standard del Q-Learning) e per aggiornare il modello interno.
    \item \textbf{Planning Loop}: nei tempi morti tra un'azione e l'altra, l'agente esegue $N$ passi di pianificazione. In ogni passo, campiona casualmente uno stato $S$ e un'azione $A$ visitati in precedenza, interroga il modello interno per ottenere il risultato simulato $(\hat{R}, \hat{S}')$ e usa queste "esperienze immaginate" per eseguire ulteriori aggiornamenti di $Q(s,a)$.
\end{enumerate}

Questo meccanismo permette di propagare le informazioni di ricompensa attraverso lo spazio degli stati molto più velocemente rispetto alla sola interazione reale, risultando estremamente efficace in ambienti deterministici o stazionari. \cite{Sutton1990}

\subsection{Monte Carlo Tree Search (MCTS)}
\label{subsec:mcts}

Sebbene nasca come algoritmo di ricerca euristica per giochi a informazione perfetta, MCTS è divenuto un pilastro del RL per la sua capacità di pianificare in spazi decisionali vasti.
Invece di apprendere una policy globale per tutto lo spazio degli stati, MCTS costruisce un albero di ricerca asimmetrico per decidere l'azione migliore dallo stato *corrente*, concentrando le risorse computazionali sulle sottostrutture più promettenti.

Il processo si articola in quattro fasi ripetute ciclicamente entro un budget di tempo:

\begin{enumerate}
    \item \textbf{Selection}: si attraversa l'albero dalla radice fino a un nodo foglia utilizzando una \textit{Tree Policy} che bilancia esplorazione e sfruttamento (tipicamente usando la formula UCB1: Upper Confidence Bound).
    \item \textbf{Expansion}: se il nodo foglia non è terminale, viene espanso aggiungendo uno o più nodi figli corrispondenti alle azioni possibili.
    \item \textbf{Simulation (Rollout)}: dal nuovo nodo, si esegue una simulazione rapida (spesso usando una policy casuale o leggera) fino a raggiungere uno stato terminale e ottenere una ricompensa cumulativa $G$.
    \item \textbf{Backpropagation}: il valore $G$ viene propagato all'indietro lungo il percorso selezionato per aggiornare le statistiche (valore medio e numero di visite) di tutti i nodi attraversati.
\end{enumerate}

Sebbene potentissimo, l'applicazione diretta di MCTS ai Sistemi di Raccomandazione è complessa a causa dell'enorme fattore di ramificazione (milioni di item possibili) e della natura stocastica del comportamento utente. \cite{Browne2012}

\subsection{Deep Model-Based RL: World Models}
\label{subsec:world-models}

Nei contesti ad alta dimensionalità (come la visione artificiale o i sistemi complessi), apprendere un modello tabellare perfetto è impossibile. L'approccio dei World Models propone di apprendere una rappresentazione compressa e latente della dinamica dell'ambiente utilizzando reti neurali.

L'architettura si divide in tre componenti:

\begin{itemize}
    \item \textbf{Vision Model (V)}: un \textit{Variational Autoencoder} (VAE) che comprime l'osservazione ad alta dimensionalità $O_t$ in un vettore latente compatto $z_t$.
    \item \textbf{Memory Model (M)}: una rete ricorrente (MDN-RNN) che apprende a predire la distribuzione di probabilità del prossimo vettore latente $z_{t+1}$ basandosi sulla storia passata e sull'azione corrente. Questo componente funge da simulatore del mondo.
    \item \textbf{Controller (C)}: una rete neurale semplice che prende in input la rappresentazione concisa del VAE ($z$) e lo stato della memoria ($h$) per decidere l'azione.
\end{itemize}

La caratteristica rivoluzionaria è che il Controller può essere addestrato interamente "dentro il sogno" (\textit{Training in Imagination}), interagendo esclusivamente con il modello simulato $M$ senza toccare l'ambiente reale. Questo permette un training massivamente parallelo e sicuro, trasferendo poi la policy appresa nel mondo reale. \cite{Ha2018}

\section{Algoritmi Model-Free Value-Based}
\label{sec:model_free_value_based}

La famiglia degli algoritmi \textit{Value-Based} mira a risolvere il problema di controllo apprendendo un'approssimazione della funzione Action-Value ottimale $Q^*(s, a)$. Una volta ottenuta una stima accurata, la policy ottima viene derivata agendo in modo \textit{greedy} rispetto ai valori Q:

\begin{equation}
    a_t = \arg\max_{a \in \mathcal{A}} Q(s_t, a)
\end{equation}

Questa classe di algoritmi è particolarmente adatta per ambienti con spazi di azione discreti, come la selezione di item nei sistemi di raccomandazione.

\subsection{Metodi Tabellari}
\label{subsec:metodi_tabellari}

Prima dell'avvento del Deep Learning, i metodi di RL memorizzavano i valori $Q$ in una tabella di lookup. Sebbene inapplicabili a problemi complessi per limiti di memoria, ne costituiscono la base teorica.

\subsubsection{SARSA (On-Policy)}

L'algoritmo \textit{SARSA} (State-Action-Reward-State-Action) apprende la funzione $Q^\pi$ della policy che l'agente sta effettivamente seguendo (inclusa l'esplorazione).
La regola di aggiornamento è:

\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
\end{equation}

Dove $A_{t+1}$ è l'azione realmente eseguita al passo successivo. Questo rende SARSA un algoritmo "prudente", che evita stati pericolosi durante l'esplorazione. \cite{Rummery1994}

\subsubsection{Q-Learning (Off-Policy)}

Il \textit{Q-Learning} è l'algoritmo capostipite dei metodi Off-Policy. Apprende direttamente il valore della policy ottima $Q^*$, indipendentemente dalle azioni effettivamente scelte dall'agente durante il training.
L'aggiornamento utilizza l'operatore di massimizzazione:

\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)]
\end{equation}

Questa proprietà è fondamentale perché permette di apprendere da dati storici o generati da policy casuali, ed è la base teorica su cui è costruito DQN. \cite{Watkins1992}

\subsection{Deep Q-Network (DQN)}
\label{subsec:dqn}

Nei sistemi reali, lo spazio degli stati è troppo vasto per una tabella. Il \textit{Deep Q-Network} (DQN) utilizza una rete neurale profonda con parametri $\theta$ per approssimare la funzione valore: $Q(s, a; \theta) \approx Q^*(s, a)$. \cite{Mnih2015}

L'addestramento avviene minimizzando l'errore quadratico medio (MSE) rispetto al target di Bellman. Tuttavia, l'uso di approssimatori non lineari nel RL è notoriamente instabile. DQN risolve questo problema introducendo due innovazioni architetturali.

\subsubsection{Experience Replay Buffer}

L'apprendimento su dati sequenziali viola l'assunzione di indipendenza (i.i.d.) necessaria per le reti neurali, poiché campioni consecutivi sono altamente correlati.
DQN utilizza un buffer di memoria $\mathcal{D}$ (\textit{Replay Memory}) per memorizzare le transizioni $(s, a, r, s')$. Durante il training, l'agente campiona un \textit{mini-batch} casuale da $\mathcal{D}$.
Questo rompe le correlazioni temporali, stabilizza il gradiente e aumenta la \textit{sample efficiency} riutilizzando i dati più volte.

\subsubsection{Target Network}

Se usassimo la stessa rete $\theta$ sia per predire il valore $Q$ che per calcolare il target $y = r + \gamma \max Q(s', a')$, il bersaglio si sposterebbe a ogni aggiornamento, causando oscillazioni.
DQN introduce una seconda rete, la **Target Network** ($\theta^-$), che viene mantenuta fissa e aggiornata periodicamente (o tramite media mobile esponenziale). La loss function diventa:

\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
\end{equation}

\subsection{Evoluzioni Architetturali (Rainbow DQN)}
\label{subsec:dqn_extensions}

Il DQN originale soffre di limitazioni come la sovrastima dei valori Q. Le moderne implementazioni integrano varianti avanzate per migliorare le prestazioni.

\subsubsection{Double DQN (DDQN)}

Il Q-Learning standard soffre di \textit{Maximization Bias}: il rumore nelle stime fa sì che l'operatore $\max$ sovrastimi sistematicamente i valori.
Il DDQN disaccoppia la selezione dell'azione dalla sua valutazione:

\begin{equation}
    Y^{DDQN} = R + \gamma Q(S', \underbrace{\arg\max_a Q(S', a; \theta)}_{\text{Selezione (Policy Net)}}; \theta^-)_{\text{Valutazione (Target Net)}}
\end{equation}

La rete principale decide qual è l'azione migliore, mentre la rete target decide quanto vale. \cite{VanHasselt2016}

\subsubsection{Dueling DQN}

In molti stati, l'azione scelta ha poco impatto sul valore complessivo. L'architettura \textit{Dueling} scompone la stima in due flussi separati:

\begin{itemize}
    \item \textbf{Value Function $V(s)$}: il valore intrinseco dello stato.
    \item \textbf{Advantage Function $A(s,a)$}: quanto l'azione $a$ è migliore rispetto alla media.
\end{itemize}

I due flussi vengono ricombinati alla fine:

\begin{equation}
    Q(s, a) = V(s) + (A(s, a) - \text{mean}_{a'} A(s, a'))
\end{equation}

Questa decomposizione permette di apprendere il valore degli stati molto più velocemente, accelerando la convergenza. \cite{Wang2016} 

\subsubsection{Prioritized Experience Replay (PER)}

Invece di campionare uniformemente, si dà priorità alle transizioni con un alto errore di predizione (\textit{TD-Error}). L'agente si "ripete" più spesso le esperienze dove ha commesso un errore grande ("sorpresa"), accelerando la convergenza su eventi rari ma significativi. \cite{Schaul2015} 

\section{Algoritmi Model-Free Policy-Based e Actor-Critic}
\label{sec:model_free_policy_based_actor_critic}

Mentre i metodi Value-Based come DQN derivano la strategia in modo indiretto (selezionando l'azione che massimizza $Q$), i metodi Policy-Based apprendono direttamente la funzione di policy parametrizzata $\pi_\theta(a|s)$.
Questo approccio offre vantaggi cruciali:

\begin{itemize}
    \item \textbf{Policy Stocastiche}: possono apprendere strategie miste ottimali e garantiscono un'esplorazione più fluida.
    \item \textbf{Efficacia in Alta Dimensionalità}: sono più adatti quando lo spazio delle azioni è continuo o molto vasto, dove il calcolo del $\max_a Q(s,a)$ diventa oneroso.
\end{itemize}

\subsection{Policy Gradient (REINFORCE)}
\label{subsec:policy_gradient}

L'algoritmo capostipite è \textit{REINFORCE}. L'obiettivo è massimizzare il ritorno atteso $J(\theta) = \mathbb{E}_{\pi_\theta}[G_t]$ aggiornando i pesi $\theta$ della rete neurale tramite ascesa del gradiente. \cite{Williams1992}

Secondo il \textit{Policy Gradient Theorem}, il gradiente è:

\begin{equation}
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi} [\nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t]
\end{equation}

Intuitivamente, questo aggiornamento aumenta la probabilità delle azioni che hanno portato a un ritorno $G_t$ elevato. Tuttavia, essendo un metodo Monte Carlo (usa il ritorno completo dell'episodio), soffre di alta varianza, rendendo il training lento e instabile. \cite{Sutton2018}

\subsection{Metodi Actor-Critic}
\label{subsec:actor_critic}

Per ridurre la varianza, i metodi \textit{Actor-Critic} combinano i vantaggi di Value-based e Policy-based utilizzando due reti neurali distinte che cooperano \cite{Konda2000}:

\begin{enumerate}
    \item \textbf{Actor (Attore) $\pi_\theta(s)$}: decide quale azione intraprendere.
    \item \textbf{Critic (Critico) $V_\phi(s)$}: stima la funzione valore dello stato corrente per giudicare la bontà dell'azione scelta dall'attore.
\end{enumerate}

L'idea chiave è sostituire il ritorno grezzo $G_t$ con la \textit{Advantage Function} $A(s,a)$, che stima quanto l'azione scelta è migliore rispetto alla media del valore dello stato:

\begin{equation}
    A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \approx (R_{t+1} + \gamma V_\phi(s_{t+1})) - V_\phi(s_t)
\end{equation}

Il termine calcolato dal Critico è noto come \textit{TD-Error}. L'Attore viene aggiornato per massimizzare questo vantaggio. Esempi storici sono \textit{A2C} e \textit{A3C} \cite{Mnih2016}.

\subsection{Proximal Policy Optimization (PPO)}
\label{subsec:ppo}

PPO è attualmente l'algoritmo \textit{On-Policy} standard nell'industria (utilizzato nella baseline PoisonRec).
Il problema dei metodi Policy Gradient standard è la sensibilità al \textit{learning rate}: un passo di aggiornamento troppo grande può portare la policy in una regione dello spazio dei parametri dove le performance crollano ("performance collapse"), rendendo impossibile il recupero. \cite{Schulman2017}

\subsubsection{Clipped Surrogate Objective}

PPO risolve questo problema introducendo un vincolo che impedisce alla nuova policy $\pi_\theta$ di discostarsi eccessivamente dalla vecchia policy $\pi_{\theta_{old}}$.
Definito il rapporto di probabilità $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$, la funzione obiettivo da massimizzare è:

\begin{equation}
    L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]
\end{equation}

La funzione $\text{clip}$ taglia il rapporto nell'intervallo $[1-\epsilon, 1+\epsilon]$ (solitamente $\epsilon=0.2$). Questo garantisce aggiornamenti conservativi e monotoni, rendendo PPO estremamente robusto.

\subsection{Soft Actor-Critic (SAC)}

Mentre PPO è On-Policy, SAC rappresenta lo stato dell'arte per il controllo continuo \textit{Off-Policy}.
Si basa sul principio della \textit{Maximum Entropy RL}: l'agente massimizza non solo il reward atteso, ma anche l'entropia della policy $\mathcal{H}(\pi(\cdot|s_t))$:

\begin{equation}
    J(\pi) = \sum_{t=0}^T \mathbb{E} [R(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))]
\end{equation}

L'entropia incoraggia l'esplorazione e impedisce alla policy di convergere prematuramente su un singolo comportamento deterministico, rendendo l'agente molto robusto a perturbazioni. \cite{Haarnoja2018}

\section{Sfide del Deep RL nei Sistemi di Raccomandazione}
\label{sec:rl_challenges}

L'applicazione del RL ai Recommender Systems presenta sfide uniche rispetto ai classici ambienti di controllo (robotica/giochi), che richiedono adattamenti specifici degli algoritmi descritti.

\subsection{Large Discrete Action Spaces}
\label{subsec:large_discrete_action_space}

In un RS, l'azione corrisponde alla scelta di un item da raccomandare o iniettare. Il catalogo può contenere milioni di item ($|\mathcal{A}| \approx 10^6 - 10^7$).
Un approccio come DQN standard, che richiede un neurone di output per ogni azione, diventa computazionalmente intrattabile.
Soluzioni comuni in letteratura includono \cite{Dulac-Arnold2016}:

\begin{itemize}
    \item \textbf{Embedding-based Action Selection}: l'agente genera un vettore continuo $a \in \mathbb{R}^d$ in uno spazio latente e l'item viene scelto tramite ricerca del vicino più prossimo ($k$-NN) nello spazio degli embedding degli item.
    \item \textbf{Candidate Generation}: l'agente RL seleziona solo tra un sottoinsieme ristretto di candidati pre-filtrati da un modello più semplice.
\end{itemize}

\subsection{Sparsità della Ricompensa e Orizzonte}
\label{subsec:sparsità_ricompensa_orizzonte}

Spesso il feedback reale (es. l'utente acquista, o l'attacco ha successo nel manipolare il ranking) arriva solo dopo una lunga sequenza di interazioni. Questo aggrava il problema dell'assegnazione del credito. Tecniche di \textit{Reward Shaping} e l'uso di modelli sequenziali a lungo raggio come \textbf{Mamba} (analizzato nel prossimo capitolo) sono essenziali per mitigare questo problema.